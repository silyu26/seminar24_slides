% This file provides an example Beamer presentation using the RWTH theme
% showcasing some of the more common options, similar to the Powerpoint version
% 12.11.2014: Revision 1 (Harold Bruintjes, Tim Lange)

% For RWTH, beamer should be loaded with class option t (top)
\documentclass[t]{beamer}
%\documentclass[t,handout]{beamer}

% Use fontspec to get Arial font
% Requires use of XeLaTeX
%\usepackage{fontspec}
%\setmainfont{Arial}
%\setsansfont{Arial}
\usepackage[scaled=.90]{helvet}
%\usepackage{helvet}
% Also force Arial for math for a more consistent look
%\usepackage{unicode-math}
%\setmathfont{Arial}

% German style date formatting (footer)
%\usepackage[ddmmyyyy]{datetime}
%\renewcommand{\dateseparator}{.}

% Format the captions used for figures etc.
\usepackage[compatibility=false]{caption}
\captionsetup{singlelinecheck=off,justification=raggedleft,labelformat=empty,labelsep=none}

% PGFPlots is used for drawing some of the charts
%\usepackage{pgfplots}
%\pgfplotsset{compat=newest}
%\input{plot_commands.tex}

\usepackage[english]{babel}

% Load the actual RWTH theme. Suggested is to load the full theme,
% as it requires some specific dimensions
\usetheme{rwth}

\setbeamercolor{math text}{fg=rwth}
\setbeamertemplate{theorems}[numbered]
\newtheorem{algorithm}[theorem]{Algorithm}

% Setup presentation information
\title{Large Language Models and Data Streams}
\subtitle{
  Seminar \textsl{Data Stream Management and Analysis}\\[1ex]
  \insertdate\\[1ex]
  \insertauthor
}
\date{\today}
\author{Silyu Li}
\institute{RWTH Aachen University}

% Set the logo to the file `logo`
% It will be scaled automatically
\logo{\includegraphics[scale=0.6]{rwth_i5_en_rgb.pdf}}

% Uncomment this if you want a TOC at every section start
%\AtBeginSection[]{
%  \begin{frame}<beamer>{Outline}
%    \tableofcontents[currentsection]
%  \end{frame}
%}
%\AtBeginSubsection[]{
%  \begin{frame}<beamer>{Outline}
%    \tableofcontents[currentsection,currentsubsection]
%  \end{frame}
%}

% Use this to control some aspects of the footer
%\setbeamertemplate{footertextextra}{Extra text in the footer\enskip|\enskip{}Extra text in the footer}
\setbeamertemplate{footertext}{%
  \insertshorttitle\\[1ex]\insertauthor}

% Title page
\setbeamercolor{title page bar}{fg=white}
\setbeamertemplate{title page}[rwth][title_small]{}



\begin{document}
\begin{frame}[plain]
  \titlepage
\end{frame}

\begin{frame}{Overview}
  \tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}{Background}
  \begin{itemize}
    \item Large language model (LLM) is already a well-known and widely-used technique.
  \end{itemize}
\end{frame}

\begin{frame}{Background}
  \begin{itemize}
    \item Large language model (LLM) is already a well-known and widely-used technique.
    \item It has significant competence and huge potential in various application fields \cite{Liu23}.
  \end{itemize}
\end{frame}

\begin{frame}{Limitation}
  \begin{itemize}
    \item However, LLMs such as ChatGPT are pre-trained on massive static datasets \cite{Gupta23}.
  \end{itemize}
\end{frame}

\section{Related works}
\begin{frame}{Large language model}
  Text goes here
\end{frame}

\begin{frame}{Data stream}
  Text goes here
\end{frame}

\section{LLM with data streams}
\subsection{Continual Learning}
\begin{frame}{Continual Learning}
  Text goes here
\end{frame}

\begin{frame}{Use Case}
  Text goes here
\end{frame}

\begin{frame}{Challenges}
  Text goes here
\end{frame}

\subsection{Prompt Engineering}
\begin{frame}{Prompt Engineering}
  Text goes here
\end{frame}

\begin{frame}{Use Case}
  Text goes here
\end{frame}

\begin{frame}{Challenges}
  Text goes here
\end{frame}

\section{Conclusion}
\begin{frame}{Background}
  Text goes here
\end{frame}

\section{References}
\begin{frame}{}
\begin{thebibliography}{}
  \bibitem{Liu23}
  Liu, Yiheng, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He et al. "Summary of chatgpt-related research and perspective towards the future of large language models." Meta-Radiology (2023): 100017.

  \bibitem{Gupta23}
  Gupta, Kshitij, Benjamin Thérien, Adam Ibrahim, Mats L. Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, and Timothée Lesort. "Continual Pre-Training of Large Language Models: How to (re) warm your model?." arXiv preprint arXiv:2308.04014 (2023).
\end{thebibliography}
\end{frame}

\end{document}
